{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTnbWDwjNqBj"
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vVIUJhUMNqBl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(models, meta_alg, data_train, targets_train, data_test, targets_test=None, random_state=None, test_size=None, cv=5):\n",
    "    \n",
    "    #Если размер тестовой выборки не задан\n",
    "    if test_size is None:\n",
    "        meta_mtrx = np.empty((targets_train.shape[0], len(models))) \n",
    "       \n",
    "    #Запускаем цикл по заполнению мета-матрицы и обучению базовых моделей на полном объеме данных\n",
    "        \n",
    "        for n, model in enumerate(models):\n",
    "            meta_mtrx[:, n] = cross_val_predict(model, data_train, \n",
    "                                                targets_train, cv=cv, \n",
    "                                                method='predict')\n",
    "            model.fit(data_train, targets_train)\n",
    "        \n",
    "        #Обучаем мета-алгоритм на данных из мета-матрицы\n",
    "        \n",
    "        meta_alg.fit(meta_mtrx, targets_train)        \n",
    "        meta_mtrx_test = np.empty((data_test.shape[0], len(models)))\n",
    "        \n",
    "        #Заполняем тестовую мета-матрицу предсказаниям базовых моделей\n",
    "        \n",
    "        for n, model in enumerate(models):\n",
    "            meta_mtrx_test[:, n] = model.predict(data_test)\n",
    "\n",
    "        #Делаем предсказания мета-алгоритма\n",
    "        \n",
    "        predicted = meta_alg.predict(meta_mtrx_test)\n",
    "\n",
    "        #Запускаем проверку: не пустая ли матрица targets_test\n",
    "        \n",
    "        if targets_test is None:\n",
    "            return predicted   \n",
    "        else:\n",
    "            print(f'{n} auc: {roc_auc_score(y_test, predicted)}')\n",
    "        \n",
    "    \n",
    "    #Если размер тестовой выборки задан то разбиваем данные\n",
    "    \n",
    "    elif test_size > 0 and test_size < 1:\n",
    "\n",
    "        \n",
    "        train, valid, train_true, valid_true = train_test_split(data_train, \n",
    "                                                    targets_train,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=0)\n",
    "        \n",
    "      #Определяем мета-матрицу и заполняем значениями в цикле   \n",
    "        meta_mtrx = np.empty((valid.shape[0], len(models))) \n",
    "      #В качестве значений мета-матрицы предсказания базовых моделей\n",
    "        for n, model in enumerate(models):\n",
    "            model.fit(data_train, targets_train)\n",
    "            meta_mtrx[:, n] = model.predict(valid)\n",
    "        #Обучаем мета-алгоритм на мета-матрице и валидирующей выборки от второго разбиения\n",
    "        meta_alg.fit(meta_mtrx, valid_true)\n",
    "    \n",
    "        meta_mtrx_test = np.empty((x_test.shape[0], len(models))) \n",
    "        for n, model in enumerate(models):\n",
    "            meta_mtrx_test[:, n] = model.predict(x_test)  \n",
    "            \n",
    "        predicted = meta_alg.predict(meta_mtrx_test)\n",
    "\n",
    "        if targets_test is None:\n",
    "            return predicted   \n",
    "        else:\n",
    "            print(f'{n} auc: {roc_auc_score(y_test, predicted)}')     \n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2kJT4LjRNqBx"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "titanic = pd.read_csv('9.7_titanic.csv')\n",
    "targets = titanic.Survived\n",
    "data = titanic.drop(columns='Survived')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                    targets,\n",
    "                                                    train_size=0.8,\n",
    "                                                    random_state=17)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "lr = LogisticRegression(random_state=17)\n",
    "svc = SVC(random_state=17)\n",
    "models = [knn, lr, svc]\n",
    "\n",
    "meta = XGBClassifier(n_estimators=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qbHZDCAiNqB0"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test_size must be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e0ba4f49e37e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstacking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a7f9d7c0a1b5>\u001b[0m in \u001b[0;36mstacking\u001b[1;34m(models, meta_alg, data_train, targets_train, data_test, targets_test, random_state, test_size, cv)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_size must be between 0 and 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: test_size must be between 0 and 1"
     ]
    }
   ],
   "source": [
    "\n",
    "stacking(models, meta, x_train, y_train, x_test, y_test, random_state=17, test_size = 0, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:00] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 auc: 0.646291031274231\n"
     ]
    }
   ],
   "source": [
    "stacking(models, meta, x_train, y_train, x_test, y_test, random_state=17, test_size = 0.3, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2 auc: 0.7601447402429569\n"
     ]
    }
   ],
   "source": [
    "stacking(models, meta, x_train, y_train, x_test, y_test, random_state=17, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:05] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking(models, meta, x_train, y_train, x_test, random_state=17, test_size = 0.3, cv=5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "jun_ml_extra_tech_stack-hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
